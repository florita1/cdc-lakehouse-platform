# WAL-CDC â†’ OLAP + Lakehouse Pipeline

This project simulates a real-time OLAP platform built on PostgreSQL WAL-based CDC, streaming through Redpanda/Debezium into ClickHouse for low-latency analytics, while dual-sinking into Iceberg/S3 for lakehouse storage. Itâ€™s deployed Kubernetes-first on AWS EKS with Terraform + Argo CD, fully observable end-to-end, and extended with Trino federation, dbt semantic models, and Flink SQL stream processing.


> Purpose-built for modern SaaS data workloads: real-time ingestion, OLAP exploration, hybrid OLAP+vector search, and automated observability.

---

## ğŸ§± Architecture

```
PostgreSQL (WAL, tuned for CDC)
     â†“
Debezium (CDC connector)
     â†“
Redpanda (Kafka-compatible broker)
     â†“
Go Ingestion Service (dual mode: synthetic | CDC)
     â†™                          â†˜
ClickHouse (ReplacingMergeTree)   Iceberg/S3 (via Flink SQL)
                â†“
         Trino Federation
                â†“
   OLAP + Lakehouse + AI/Vector Search
```

### Component Summary

| Component               | Role                                                                                   |
|--------------------------|----------------------------------------------------------------------------------------|
| **PostgreSQL**           | Source of truth; WAL tuned for logical replication                                     |
| **Debezium**             | Captures WAL changes and publishes CDC events into Redpanda                            |
| **Redpanda**             | Kafka-compatible message broker for buffering and fan-out of CDC streams               |
| **Go Ingestion Service** | Dual-mode: generates synthetic events or consumes Debezium CDC events and inserts into sinks |
| **ClickHouse**           | Real-time OLAP storage; `ReplacingMergeTree` tables for deduplication and versioning    |
| **Flink SQL**            | Processes Debezium CDC with event-time semantics, de-dupe, and exactly-once writes into Iceberg |
| **Iceberg/S3**           | Lakehouse storage; durable, append-only tables for federated querying                   |
| **Trino**                | Federated SQL query layer across PostgreSQL, ClickHouse, and Iceberg/S3                 |
| **dbt**                  | Models, semantic layers, incremental marts, and data quality tests (on ClickHouse/Trino)|
| **Observability Stack**  | Prometheus SDK + Grafana Alloy (metrics), Tempo (traces), Loki (logs), Pixie (live debugging), VictoriaMetrics (retention) |
| **Argo CD / Terraform**  | GitOps + Infrastructure as Code for Kubernetes-first provisioning and lifecycle management |
| **KEDA**                 | Autoscaling CDC consumers based on Redpanda lag                                         |


---

## ğŸ”§ Infrastructure

- **Cluster Name:** `postgres-wal-cdc-cluster`
- **Provisioning:** Terraform (modular, destroyable)
- **Delivery:** GitOps via Argo CD
- **Infrastructure Stack:**
  - VPC
  - EKS
  - IAM
  - Argo CD
- **Teardown:** Full `terraform destroy` support

---
## ğŸš¦ Argo CD Setup

This project uses Argo CD as the GitOps controller to deploy and manage all workloads in a **controlled, dependency-aware sequence**.

### **App of Apps pattern**
- The **`apps/root.yaml`** manifest is the â€œrootâ€ Argo CD Application.
- Syncing this single application cascades into all other component applications (`postgres.yaml`, `redpanda.yaml`, `debezium.yaml`, `clickhouse.yaml`, etc.).
- Centralizes version control for the entire CDC stack.

### **Sync Waves**
- Components deploy in **numbered sync waves** to ensure dependencies are ready before dependents run:
  - **Wave 0:** Namespace creation (`wal-cdc-namespaces` job)
  - **Wave 1:** Postgres
  - **Wave 2:** Stateful services (Redpanda)
  - **Wave 3:** Connectors (Debezium)
  - **Wave 4:** Clickhouse Operator
  - **Wave 5:** ClickHouseInstallation

### **Namespace Bootstrap Job**
- `apps/wal-cdc-namespaces.yaml` runs as a pre-step to create all required namespaces (`postgres`, `redpanda`, `debezium`, `clickhouse`) before any Helm releases or Kustomize deployments.
- Prevents race conditions where Argo CD tries to deploy workloads into namespaces that donâ€™t yet exist.

### **Post-Sync Hooks**
- **Debezium** uses a PostSync hook to register the PostgreSQL connector only after the Kafka Connect workload is healthy.
  - The hook (*debezium/job-register-connector.yaml*) applies the connector configuration from *configmap-connector.json.yaml*.
  - This ensures the connector is created automatically and reliably, without requiring any manual registration steps.

### **Benefits**
- Declarative, reproducible environment setup
- Automated dependency ordering via sync waves
- Fully bootstrapped namespaces and DB initialization without manual steps
- One-click/full-stack deployment from the Argo CD UI

---
## ğŸ“¦ Project Structure
```
wal-cdc-platform/
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ apps/                              # Argo CD Application CRs (App-of-Apps model)
â”‚   â”œâ”€â”€ clickhouse-operator.yaml
â”‚   â”œâ”€â”€ clickhouse.yaml
â”‚   â”œâ”€â”€ debezium.yaml
â”‚   â”œâ”€â”€ flink.yaml
â”‚   â”œâ”€â”€ grafana.yaml
â”‚   â”œâ”€â”€ postgres.yaml
â”‚   â”œâ”€â”€ redpanda.yaml
â”‚   â”œâ”€â”€ root.yaml
â”‚   â”œâ”€â”€ trino.yaml
â”‚   â”œâ”€â”€ vector-search.yaml
â”‚   â””â”€â”€ wal-cdc-namespaces.yaml
â”‚
â”œâ”€â”€ clickhouse/                        # Altinity Operator CRDs + init SQL
â”‚   â”œâ”€â”€ clickhouseinstallation.yaml
â”‚   â”œâ”€â”€ init-configmap.yaml
â”‚   â””â”€â”€ init-job.yaml
â”‚
â”œâ”€â”€ dbt/                               # dbt project for models, marts, semantic layers
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ flink/                             # Flink SQL + jobs for Iceberg sink
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ normalize-cdc.sql
â”‚   â”‚   â””â”€â”€ dedupe-stream.sql
â”‚   â””â”€â”€ flinkdeployment.yaml
â”‚
â”œâ”€â”€ ingestion-service/                 # Go ingestion service (dual-mode: synthetic + CDC)
â”‚   â”œâ”€â”€ charts/                        # Helm chart
â”‚   â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ main.go
â”‚
â”œâ”€â”€ kustomize/                         # Base configs for Debezium + Postgres
â”‚   â”œâ”€â”€ debezium/
â”‚   â”‚   â”œâ”€â”€ configmap-connector.json.yaml
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ job-register-connector.yaml
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ secret-postgres.yaml
â”‚   â”‚   â””â”€â”€ service.yaml
â”‚   â””â”€â”€ postgres/
â”‚       â”œâ”€â”€ configmap-init.sql.yaml
â”‚       â”œâ”€â”€ deployment.yaml
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â””â”€â”€ service.yaml
â”‚
â”œâ”€â”€ namespaces/                        # Kubernetes namespaces for operators + apps
â”‚   â”œâ”€â”€ clickhouse-operator.yaml
â”‚   â”œâ”€â”€ clickhouse.yaml
â”‚   â”œâ”€â”€ debezium.yaml
â”‚   â”œâ”€â”€ flink.yaml
â”‚   â”œâ”€â”€ observability.yaml
â”‚   â”œâ”€â”€ postgres.yaml
â”‚   â””â”€â”€ redpanda.yaml
â”‚
â”œâ”€â”€ observability/                     # Monitoring + tracing + logging
â”‚   â”œâ”€â”€ alloy/                         # Grafana Alloy configs
â”‚   â”œâ”€â”€ grafana/                       # Dashboards + provisioning
â”‚   â”œâ”€â”€ loki/                          # Logging stack
â”‚   â”œâ”€â”€ tempo/                         # Distributed tracing
â”‚   â”œâ”€â”€ victoria-metrics/              # Long-term metrics storage
â”‚   â””â”€â”€ pixie/                         # Live Kubernetes debugging
â”‚
â””â”€â”€ terraform/                         # Infra as Code (AWS EKS + networking)
    â”œâ”€â”€ environments/
    â”‚   â””â”€â”€ dev/
    â”‚       â”œâ”€â”€ argocd.tf
    â”‚       â”œâ”€â”€ eks.tf
    â”‚       â”œâ”€â”€ iam.tf
    â”‚       â”œâ”€â”€ providers.tf
    â”‚       â”œâ”€â”€ variables.tf
    â”‚       â””â”€â”€ vpc.tf
    â”‚
    â””â”€â”€ modules/
        â”œâ”€â”€ argocd/
        â”‚   â”œâ”€â”€ main.tf
        â”‚   â”œâ”€â”€ outputs.tf
        â”‚   â””â”€â”€ values.yaml
        â”œâ”€â”€ eks/
        â”‚   â”œâ”€â”€ main.tf
        â”‚   â”œâ”€â”€ outputs.tf
        â”‚   â””â”€â”€ variables.tf
        â”œâ”€â”€ iam/
        â”‚   â”œâ”€â”€ main.tf
        â”‚   â”œâ”€â”€ outputs.tf
        â”‚   â””â”€â”€ variables.tf
        â””â”€â”€ vpc/
            â”œâ”€â”€ main.tf
            â”œâ”€â”€ outputs.tf
            â””â”€â”€ variables.tf

```

---

## ğŸ” Under the Hood â€” How It Works

This project simulates a real-time OLAP + Lakehouse pipeline using PostgreSQL WAL-based change data capture, Redpanda buffering, and a dual-mode Go ingestion service that writes into ClickHouse for OLAP and Iceberg/S3 for lakehouse storage. The entire stack is deployed Kubernetes-first via GitOps on AWS EKS.
Infrastructure is provisioned with Terraform (EKS cluster, VPC, IAM, Argo CD) using modular code and remote state. Supports full teardown with terraform destroy.
- PostgreSQL is patched for logical replication; inserts/updates emit WAL changes.
- Debezium streams those WAL changes into Redpanda using Kafka-compatible protocols.
- Redpanda buffers CDC streams and fans out events for downstream consumers.
- Go Ingestion Service supports two modes:
     - mode=synthetic (default): generates mock UserEvent payloads for testing and observability.
     - mode=cdc: consumes Debezium envelopes from Redpanda, normalizes into UserEvent structs, and inserts into sinks.
- ClickHouse stores CDC events in ReplacingMergeTree tables for deduplication, versioning, and low-latency OLAP queries.
- Flink SQL processes Debezium CDC with event-time watermarks, PK-based de-dupe, and exactly-once writes into Iceberg/S3.
- Iceberg/S3 acts as the lakehouse layer for durable storage and federated queries.
- Trino unifies queries across PostgreSQL, ClickHouse, and Iceberg, enabling hybrid OLTPâ€“OLAP analysis and vector/semantic queries.
- dbt layers semantic models, marts, and data tests on top of ClickHouse/Trino.
- Observability is first-class:
Metrics (Prometheus SDK â†’ Grafana Alloy â†’ VictoriaMetrics)
Traces (OpenTelemetry â†’ Tempo)
Logs (structured logs â†’ Alloy â†’ Loki)
Live debugging (Pixie in-cluster).
- KEDA autoscaling adjusts ingestion service replicas based on Redpanda lag.
- Helm charts manage all workloads (ClickHouse, Debezium, Redpanda, Flink, Trino, ingestion service, observability) and are delivered declaratively via Argo CD.

---

## ğŸ–¥ï¸ CDC Verification

![Argo CD UI Applications](screenshots/argocd.png)


### **1. PostgreSQL WAL Settings**
Snippet from kustomize/postgres/deployment.yaml
```bash
          args:
            - "-c"
            - "wal_level=logical"
            - "-c"
            - "max_wal_senders=10"
            - "-c"
            - "max_replication_slots=10"
```

---

### **2. Verify Debezium Connector Status**
```bash
# Replace host with your Debezium service DNS or port-forwarded localhost
curl -s http://connect:8083/connectors/postgres-appdb-connector/status
```
Expected RUNNING status:
``` json
{
  "name": "postgres-appdb-connector",
  "connector": {
    "state": "RUNNING",
    "worker_id": "10.2.0.250:8083"
  },
  "tasks": [
    {
      "id": 0,
      "state": "RUNNING",
      "worker_id": "10.2.0.250:8083"
    }
  ],
  "type": "source"
}
```

---

### **3. Verify Redpanda CDC Events**
```bash
# Consume a few messages from the CDC topic
kubectl -n redpanda run kafkactl --restart=Never -it --image=bitnami/kafka:3.7.0 -- \                         
  kafka-topics.sh --list --bootstrap-server redpanda.redpanda.svc.cluster.local:9093
```
Example output:
```json
{
  "topic": "dbserver1.app.users",
  "key": "{\"id\":2}",
  "value": "{\"before\":null,\"after\":{\"id\":2,\"name\":\"TestUser\",\"email\":\"test+upd@example.com\"},\"source\":{\"version\":\"2.6.2.Final\",\"connector\":\"postgresql\",\"name\":\"dbserver1\",\"ts_ms\":1754816578832,\"snapshot\":\"false\",\"db\":\"appdb\",\"sequence\":\"[\\\"26619888\\\",\\\"26619888\\\"]\",\"ts_us\":1754816578832901,\"ts_ns\":1754816578832901000,\"schema\":\"app\",\"table\":\"users\",\"txId\":759,\"lsn\":26619888,\"xmin\":null},\"op\":\"u\",\"ts_ms\":1754816579296,\"ts_us\":1754816579296122,\"ts_ns\":1754816579296122309,\"transaction\":null}",
  "timestamp": 1754816579456,
  "partition": 0,
  "offset": 2
}
```

---

## ğŸ”‘ Helpful Commands

### Get Argo CD UI Password
```bash
kubectl -n wal-cdc-argocd get secret argocd-initial-admin-secret   -o jsonpath='{.data.password}' | base64 -d; echo
```
